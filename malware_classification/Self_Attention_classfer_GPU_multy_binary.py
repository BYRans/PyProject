# %%
from keras.utils import np_utils
import matplotlib.pyplot as plt
import time
import datetime
from malware_classification.Self_Attention import Self_Attention_Layer
from malware_classification import common_process_data as read_data
from malware_classification import global_var as GLVAR
from keras.models import Model
from keras.layers import *
import keras
import tensorflow as tf
import keras_metrics

import os


def show_train_history(train_history, train, validation, epochs):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    title = 'Train History of Self-Attention: epochs-' + str(epochs) + " " + str(
        time.strftime("%Y-%m-%d %X", time.localtime()))
    plt.title(title)
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()


def train_self_attention(lable_name, epochs, batch_size):
    print("================training ", lable_name, ".......================")

    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    gpu_options = tf.GPUOptions(allow_growth=True)
    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
    keras.backend.set_session(sess)

    max_features = GLVAR.TOTAL_OPERATIONS_COUNT + 1  # 该数要比operation的个数大1

    print('Loading data...')

    current_lable_data = GLVAR.MULTY_BINARY_TRAIN_AND_TEST_DATA_DIR + lable_name + '.npz'
    print("Current lable train and test data dir is : %s"%(current_lable_data))
    (x_train, y_train), (x_test, y_test) = read_data.load_npz_data(current_lable_data)
    x_train = x_train.reshape(-1, GLVAR.pic_pow_size * GLVAR.pic_pow_size)  # why / 255?
    x_test = x_test.reshape(-1, GLVAR.pic_pow_size * GLVAR.pic_pow_size)
    # 标签转换为独热码
    y_train = np_utils.to_categorical(y_train, num_classes=GLVAR.NUM_CLASSES_OF_MULTY_BINARY)
    y_test = np_utils.to_categorical(y_test, num_classes=GLVAR.NUM_CLASSES_OF_MULTY_BINARY)
    print(len(x_train), 'train sequences')
    print(len(x_test), 'test sequences')

    # %%数据归一化处理

    maxlen = GLVAR.pic_pow_size * GLVAR.pic_pow_size

    print('x_train shape:', x_train.shape)

    print('x_test shape:', x_test.shape)

    S_inputs = Input(shape=(maxlen,), dtype='int32')

    embeddings = Embedding(max_features, 256)(S_inputs)

    O_seq = Self_Attention_Layer(256)(embeddings)

    O_seq = GlobalAveragePooling1D()(O_seq)

    O_seq = Dropout(0.5)(O_seq)

    O_seq = Dense(16, activation='softmax')(O_seq)

    outputs = Dense(GLVAR.NUM_CLASSES_OF_MULTY_BINARY, activation='softmax')(O_seq)

    model = Model(inputs=S_inputs, outputs=outputs)

    print(model.summary())

    # try using different optimizers and different optimizer configs
    recall = keras_metrics.binary_recall(label=0)
    # 使用适于二分类的loss函数 binary_crossentropy
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', recall])

    # %%
    print('Training')

    h = model.fit(x_train, y_train,
                  batch_size=batch_size,
                  epochs=epochs,
                  validation_data=(x_test, y_test))

    model_filename = GLVAR.MULTY_BINARY_SELF_ATTENTION_MODEL_DIR + lable_name + ".h5"
    model.save(model_filename)

    plt.plot(h.history["loss"], label="train_loss")
    plt.plot(h.history["val_loss"], label="val_loss")
    plt.plot(h.history["acc"], label="train_acc")
    plt.plot(h.history["val_acc"], label="val_acc")
    plt.legend()
    plt.show()

    print("-----------------------DY Add------------------------")

    show_train_history(h, 'acc', 'val_acc', epochs)
    show_train_history(h, 'loss', 'val_loss', epochs)

    print('Testing--------------')
    loss, accuracy, recall = model.evaluate(x_test, y_test,batch_size=batch_size)

    print('test loss:', loss)
    print('test accuracy:', accuracy)
    print('test recall:', recall)

    print("\t[Info] Accuracy of testing data = {:2.1f}%".format(accuracy * 100.0))


def main():
    epochs = 50
    batch_size = 32
    lable_list = {"Trojan", "Backdoor", "Downloader", "Worms", "Spyware", "Adware", "Dropper",
                  "Virus"}
    for lable in lable_list:
        starttime = datetime.datetime.now()
        train_self_attention(lable, epochs, batch_size)
        runtime = (datetime.datetime.now() - starttime).seconds
        print("--------Train %s runtime is: %d seconds.---------" % (lable, runtime))


if __name__ == '__main__':
    main()
