import numpy as np
np.random.seed(1337)
import time
from keras.utils import np_utils
from keras.layers import *
from keras.models import *
from malware_classification import common_process_data as read_data
from keras.layers import Input, Conv2D
from malware_classification.Self_Attention import Self_Attention_Layer
from malware_classification import global_var as GLVAR
import keras_metrics
import keras
import tensorflow as tf
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
gpu_options = tf.GPUOptions(allow_growth=True)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
keras.backend.set_session(sess)


batch_size = 64
epochs = 30

def model_preout_np(lable_name,x_train,x_test):
    print("===========running ",lable_name," predict of preout layer.....===========")
    model_filename = GLVAR.MULTY_BINARY_SELF_ATTENTION_MODEL_DIR + lable_name + ".h5"
    print("load model:",model_filename)

    recall = keras_metrics.binary_recall(label=0)
    trained_model = load_model(model_filename,
                              custom_objects={'Self_Attention_Layer': Self_Attention_Layer,'binary_recall': recall})
    # trained_model.summary()
    trained_preout_model = Model(inputs=trained_model.input,
                                outputs=trained_model.layers[4].output)
    print("running ", lable_name, " x_train predict of preout layer.....")
    x_train_tmp = trained_preout_model.predict(x_train)
    print("running ", lable_name, " x_test predict of preout layer.....")
    x_test_tmp = trained_preout_model.predict(x_test)
    x_train_tmp = x_train_tmp.reshape(-1, 256)
    x_test_tmp = x_test_tmp.reshape(-1, 256)
    print("x_train_tmp shape is:",x_train_tmp.shape)
    print("x_test_tmp shape is:", x_test_tmp.shape)
    return x_train_tmp,x_test_tmp

def model_predict_lable_np(lable_name,x_train,x_test):
    print("===========running ",lable_name," Self Attention predict .....===========")
    model_filename = GLVAR.MULTY_BINARY_SELF_ATTENTION_MODEL_DIR + lable_name + ".h5"
    print("load model:",model_filename)

    recall = keras_metrics.binary_recall(label=0)
    trained_model = load_model(model_filename,
                              custom_objects={'Self_Attention_Layer': Self_Attention_Layer,'binary_recall': recall})
    # trained_model.summary()
    trained_sa_model = Model(inputs=trained_model.input,
                                outputs=trained_model.output)
    print("running ", lable_name, " x_train predict .....")
    x_train_result = trained_sa_model.predict(x_train)
    print("running ", lable_name, " x_test predict .....")
    x_test_result = trained_sa_model.predict(x_test)

    with open(GLVAR.LABLE_INDEX_FINAME) as raw_data:
        lables_set_index = {}
        for line in raw_data:
            lable_index = line.replace('\n', '').split('-')
            lables_set_index[lable_index[0]] = lable_index[1]

    times=2
    x_train_result_tmp = np.where(x_train_result[:, 0] < x_train_result[:, 1], lables_set_index[lable_name], -1)
    x_train_result_repet = np.repeat(x_train_result_tmp, GLVAR.NUM_CLASSES*times).reshape(-1, GLVAR.NUM_CLASSES*times)
    x_test_result_tmp = np.where(x_test_result[:, 0] < x_test_result[:, 1], lables_set_index[lable_name], -1)
    x_test_result_repet = np.repeat(x_test_result_tmp,GLVAR.NUM_CLASSES*times).reshape(-1,GLVAR.NUM_CLASSES*times)


    # x_train_result = np.repeat(x_train_result,4).reshape(-1, GLVAR.NUM_CLASSES)
    # x_test_result = np.repeat(x_test_result,4).reshape(-1, GLVAR.NUM_CLASSES)
    print("x_train_tmp shape is:",x_train_result_repet.shape)
    print("x_test_tmp shape is:", x_test_result_repet.shape)
    return x_train_result_repet,x_test_result_repet

# data pre-processing
(x_train, y_train), (x_test, y_test) = read_data.load_npz_data(GLVAR.TRAIN_AND_TEST_DATA)
x_train = x_train.reshape(-1,GLVAR.pic_pow_size*GLVAR.pic_pow_size)
x_test = x_test.reshape(-1,GLVAR.pic_pow_size*GLVAR.pic_pow_size)
y_train = np_utils.to_categorical(y_train, num_classes=GLVAR.NUM_CLASSES)
y_test = np_utils.to_categorical(y_test, num_classes=GLVAR.NUM_CLASSES)
print('X_train shape:', x_train.shape)
print('X_test shape:', x_test.shape)

lable_list = {"Trojan","Backdoor","Downloader","Worms","Spyware","Adware","Dropper","Virus"}
cnn_x_train=np.zeros(shape=(0,len(x_train),256+GLVAR.NUM_CLASSES))
cnn_x_test=np.zeros(shape=(0,len(x_test),256+GLVAR.NUM_CLASSES))
for lable in lable_list:
    x_train_tmp,x_test_tmp = model_preout_np(lable, x_train, x_test) # Self Attention preout data
    if len(cnn_x_train) == 0:
        cnn_x_train = x_train_tmp
    else:
        cnn_x_train = np.hstack((cnn_x_train,x_train_tmp))
    if len(cnn_x_test) == 0:
        cnn_x_test = x_test_tmp
    else:
        cnn_x_test = np.hstack((cnn_x_test,x_test_tmp))

    x_train_result,x_test_result = model_predict_lable_np(lable, x_train, x_test) # Self Attention predict result
    cnn_x_train = np.hstack((cnn_x_train, x_train_result))
    cnn_x_test = np.hstack((cnn_x_test, x_test_result))

    print("cnn_x_train shape is:",cnn_x_train.shape)
    print("cnn_x_test shape is:", cnn_x_test.shape)
    print("cnn_x_train length is:", len(cnn_x_train.shape))
    print("cnn_x_test length is:", len(cnn_x_test.shape))

m = 64
n = 34
cnn_x_train = cnn_x_train.reshape(-1,m,n,1)
cnn_x_test = cnn_x_test.reshape(-1,m,n,1)

print(cnn_x_train.shape)
print(cnn_x_test.shape)

np.savez(GLVAR.MULTY_BINARY_CNN_TRAIN_TEST_DATA, x_train=cnn_x_train, x_test=cnn_x_test, y_train=y_train,y_test=y_test)

(cnn_x_train, y_train), (cnn_x_test, y_test) = read_data.load_npz_data(GLVAR.MULTY_BINARY_CNN_TRAIN_TEST_DATA)

cnn_x_train = cnn_x_train.reshape(-1,m,n,1)
cnn_x_test = cnn_x_test.reshape(-1,m,n,1)

inputs = Input(shape=(m,n,1))
# build CNN model linked RNN outputs
x = Conv2D(filters=128,
           kernel_size=(3,3),
           padding='same',
           input_shape=(m,n,1),
           activation='relu',
           name='conv2d_1')(inputs)
# x = AveragePooling2D(pool_size=2, name='max_pooling2d_1')(x)
x = Conv2D(filters=64,
           kernel_size=(3,3),
           padding='same',
           input_shape=(m,n,1),
           activation='relu',
           name='conv2d_2')(x)
# x = AveragePooling2D(pool_size=2, name='max_pooling2d_2')(x)
x = Conv2D(filters=32,
           kernel_size=(3,3),
           padding='same',
           input_shape=(m,n,1),
           activation='softmax',
           name='conv2d_3')(x)
# x = AveragePooling2D(pool_size=2, name='max_pooling2d_3')(x)
# x = Dropout(0.25)(x)
x = Flatten()(x)
# x = Dense(128)(x)
# x = Dropout(0.5)(x)
output = Dense(GLVAR.NUM_CLASSES, activation='softmax')(x)
model = Model(inputs=inputs, outputs=output)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
print(model.summary())

print('Training------------')
train_history = model.fit(cnn_x_train, y_train,validation_data=(cnn_x_test,y_test), epochs=epochs, batch_size=batch_size)

print('Testing--------------')
loss, accuracy = model.evaluate(cnn_x_test, y_test)

print('test loss:', loss)
print('test accuracy:', accuracy)


print("-----------------------DY Add------------------------")
import matplotlib.pyplot as plt


def show_train_history(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    title = 'Train History of Trained_Self_Attention-CNN: epochs-' + str(epochs) + " " + str(time.strftime("%Y-%m-%d %X", time.localtime()))
    plt.title(title)
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()


show_train_history(train_history, 'acc', 'val_acc')

show_train_history(train_history, 'loss', 'val_loss')

print()
print("\t[Info] Accuracy of testing data = {:2.1f}%".format(accuracy * 100.0))


